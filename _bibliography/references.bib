@article{sharan_point_2021,
	title = {Point detection through multi-instance deep heatmap regression for sutures in endoscopy},
	volume = {16},
	copyright = {All rights reserved},
	issn = {1861-6429},
	url = {https://doi.org/10.1007/s11548-021-02523-w},
	doi = {10.1007/s11548-021-02523-w},
	abstract = {Mitral valve repair is a complex minimally invasive surgery of the heart valve. In this context, suture detection from endoscopic images is a highly relevant task that provides quantitative information to analyse suturing patterns, assess prosthetic configurations and produce augmented reality visualisations. Facial or anatomical landmark detection tasks typically contain a fixed number of landmarks, and use regression or fixed heatmap-based approaches to localize the landmarks. However in endoscopy, there are a varying number of sutures in every image, and the sutures may occur at any location in the annulus, as they are not semantically unique.},
	language = {en},
	number = {12},
	urldate = {2022-02-18},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Sharan, Lalith and Romano, Gabriele and Brand, Julian and Kelm, Halvar and Karck, Matthias and De Simone, Raffaele and Engelhardt, Sandy},
	month = dec,
	year = {2021},
	pages = {2107--2117},
	file = {Springer Full Text PDF:/home/lalith/Zotero/storage/DQHE3VQL/Sharan et al. - 2021 - Point detection through multi-instance deep heatma.pdf:application/pdf;Springer Full Text PDF:/home/lalith/Zotero/storage/K377C2I4/Sharan et al. - 2021 - Point detection through multi-instance deep heatma.pdf:application/pdf},
}

@article{sharan_mutually_2022,
	title = {Mutually {Improved} {Endoscopic} {Image} {Synthesis} and {Landmark} {Detection} in {Unpaired} {Image}-to-{Image} {Translation}},
	volume = {26},
	copyright = {All rights reserved},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2021.3099858},
	abstract = {The CycleGAN framework allows for unsupervised image-to-image translation of unpaired data. In a scenario of surgical training on a physical surgical simulator, this method can be used to transform endoscopic images of phantoms into images which more closely resemble the intra-operative appearance of the same surgical target structure. This can be viewed as a novel augmented reality approach, which we coined Hyperrealism in previous work. In this use case, it is of paramount importance to display objects like needles, sutures or instruments consistent in both domains while altering the style to a more tissue-like appearance. Segmentation of these objects would allow for a direct transfer, however, contouring of these, partly tiny and thin foreground objects is cumbersome and perhaps inaccurate. Instead, we propose to use landmark detection on the points when sutures pass into the tissue. This objective is directly incorporated into a CycleGAN framework by treating the performance of pre-trained detector models as an additional optimization goal. We show that a task defined on these sparse landmark labels improves consistency of synthesis by the generator network in both domains. Comparing a baseline CycleGAN architecture to our proposed extension (DetCycleGAN), mean precision (PPV) improved by \$+61.32\$, mean sensitivity (TPR) by \$+37.91\$, and mean \$F\_1\$ score by \$+0.4743\$. Furthermore, it could be shown that by dataset fusion, generated intra-operative images can be leveraged as additional training data for the detection network itself.},
	number = {1},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Sharan, Lalith and Romano, Gabriele and Koehler, Sven and Kelm, Halvar and Karck, Matthias and De Simone, Raffaele and Engelhardt, Sandy},
	month = jan,
	year = {2022},
	note = {},
	keywords = {Generative adversarial networks, Training, Task analysis, Semantics, CycleGAN, Generative Adversarial Networks, Landmark Detection, Landmark Localization, Maintenance engineering, Mitral Valve Repair, Surgery, Surgical Simulation, Surgical Training, Valves, landmark detection, landmark localization, mitral valve repair, surgical simulation, surgical training},
	pages = {127--138},
	file = {IEEE Xplore Abstract Record:/home/lalith/Zotero/storage/XU4WGUHD/9496194.html:text/html;IEEE Xplore Full Text PDF:/home/lalith/Zotero/storage/FPCFCIPT/Sharan et al. - 2021 - Mutually improved endoscopic image synthesis and l.pdf:application/pdf;IEEE Xplore Full Text PDF:/home/lalith/Zotero/storage/KKCDB3P8/Sharan et al. - 2022 - Mutually Improved Endoscopic Image Synthesis and L.pdf:application/pdf;Submitted Version:/home/lalith/Zotero/storage/57MENPMY/Sharan et al. - 2022 - Mutually Improved Endoscopic Image Synthesis and L.pdf:application/pdf},
}


@brkarticle{sharan_mvhota_2023,
	title = {{mvHOTA}: {A} multi-view higher order tracking accuracy metric to measure temporal and spatial associations in multi-point tracking},
	volume = {11},
	issn = {2168-1163, 2168-1171},
	shorttitle = {{mvHOTA}},
	url = {https://www.tandfonline.com/doi/full/10.1080/21681163.2022.2159535},
	doi = {10.1080/21681163.2022.2159535},
	language = {en},
	number = {4},
	urldate = {2023-11-02},
	journal = {Computer Methods in Biomechanics and Biomedical Engineering: Imaging \& Visualization},
	author = {Sharan, Lalith and Kelm, Halvar and Romano, Gabriele and Karck, Matthias and De Simone, Raffaele and Engelhardt, Sandy},
	month = jul,
	year = {2023},
	pages = {1281--1289},
	file = {Available Version (via Google Scholar):/home/lalith/Zotero/storage/3TYHV6RC/Sharan et al. - 2023 - mvHOTA A multi-view higher order tracking accurac.pdf:application/pdf},
}


@incollection{burger_comparison_2022,
	address = {Wiesbaden},
	title = {Comparison of {Depth} {Estimation} {Setups} from {Stereo} {Endoscopy} and {Optical} {Tracking} for {Point} {Measurements}},
	isbn = {978-3-658-36932-3},
	url = {https://link.springer.com/10.1007/978-3-658-36932-3_35},
	language = {de},
	urldate = {2023-11-02},
	booktitle = {Bildverarbeitung für die {Medizin} 2022},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Burger, Lukas and Sharan, Lalith and Fischer, Samantha and Brand, Julian and Hehl, Maximillian and Romano, Gabriele and Karck, Matthias and De Simone, Raffaele and Wolf, Ivo and Engelhardt, Sandy},
	editor = {Maier-Hein, Klaus and Deserno, Thomas M. and Handels, Heinz and Maier, Andreas and Palm, Christoph and Tolxdorff, Thomas},
	year = {2022},
	doi = {10.1007/978-3-658-36932-3_35},
	note = {Series Title: Informatik aktuell},
	pages = {160--165},
	file = {Available Version (via Google Scholar):/home/lalith/Zotero/storage/8CHL25ZR/Burger et al. - 2022 - Comparison of Depth Estimation Setups from Stereo .pdf:application/pdf},
}

@incollection{koehler_comparison_2022-1,
	address = {Wiesbaden},
	title = {Comparison of {Evaluation} {Metrics} for {Landmark} {Detection} in {CMR} {Images}},
	isbn = {978-3-658-36932-3},
	url = {https://link.springer.com/10.1007/978-3-658-36932-3_43},
	language = {de},
	urldate = {2023-11-02},
	booktitle = {Bildverarbeitung für die {Medizin} 2022},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Koehler, Sven and Sharan, Lalith and Kuhm, Julian and Ghanaat, Arman and Gordejeva, Jelizaveta and Simon, Nike K. and Grell, Niko M. and {André}, Florian and Engelhardt, Sandy},
	editor = {Maier-Hein, Klaus and Deserno, Thomas M. and Handels, Heinz and Maier, Andreas and Palm, Christoph and Tolxdorff, Thomas},
	year = {2022},
	doi = {10.1007/978-3-658-36932-3_43},
	note = {Series Title: Informatik aktuell},
	pages = {198--203},
	file = {Available Version (via Google Scholar):/home/lalith/Zotero/storage/4AK8LP3I/Koehler et al. - 2022 - Comparison of Evaluation Metrics for Landmark Dete.pdf:application/pdf},
}


@inproceedings{engelhardt_cross-domain_2019,
	address = {Cham},
	title = {Cross-{Domain} {Conditional} {Generative} {Adversarial} {Networks} for {Stereoscopic} {Hyperrealism} in {Surgical} {Training}},
	isbn = {978-3-030-32254-0},
	doi = {10.1007/978-3-030-32254-0_18},
	abstract = {Phantoms for surgical training are able to mimic cutting and suturing properties and patient-individual shape of organs, but lack a realistic visual appearance that captures the heterogeneity of surgical scenes. In order to overcome this in endoscopic approaches, hyperrealistic concepts have been proposed to be used in an augmented reality-setting, which are based on deep image-to-image transformation methods. Such concepts are able to generate realistic representations of phantoms learned from real intraoperative endoscopic sequences. Conditioned on frames from the surgical training process, the learned models are able to generate impressive results by transforming unrealistic parts of the image (e.g. the uniform phantom texture is replaced by the more heterogeneous texture of the tissue). Image-to-image synthesis usually learns a mapping \$\$G:X{\textasciitilde}{\textbackslash}rightarrow {\textasciitilde}Y\$\$ such that the distribution of images from G(X) is indistinguishable from the distribution Y. However, it does not necessarily force the generated images to be consistent and without artifacts. In the endoscopic image domain this can affect depth cues and stereo consistency of a stereo image pair, which ultimately impairs surgical vision. We propose a cross-domain conditional generative adversarial network approach (GAN) that aims to generate more consistent stereo pairs. The results show substantial improvements in depth perception and realism evaluated by 3 domain experts and 3 medical students on a 3D monitor over the baseline method. In 84 of 90 instances our proposed method was preferred or rated equal to the baseline.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2019},
	publisher = {Springer International Publishing},
	author = {Engelhardt, Sandy and Sharan, Lalith and Karck, Matthias and Simone, Raffaele De and Wolf, Ivo},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	year = {2019},
	keywords = {Augmented reality, Generative adversarial networks, Laparoscopy, Minimally-invasive surgical training, Mitral valve simulator},
	pages = {155--163},
	file = {Submitted Version:/home/lalith/Zotero/storage/N2DEJU58/Engelhardt et al. - 2019 - Cross-Domain Conditional Generative Adversarial Ne.pdf:application/pdf},
}